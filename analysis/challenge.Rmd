---
title: "Reproducibility challenge"
author: "John Blischak"
date: "2020-06-11"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## Introduction

For the reproducibility challenge, you will attempt to re-run an analysis of
Spotify song genres that was inspired by the blog post 
[Understanding + classifying genres using Spotify audio features][blog-post]
by Kaylin Pavlik ([\@kaylinquest][kaylinquest]).

[kaylinquest]: https://twitter.com/kaylinquest
[blog-post]: https://www.kaylinpavlik.com/classifying-songs-genres/

<blockquote class="twitter-tweet" data-dnt="true"><p lang="en" dir="ltr">NEW: Understanding song genres using Spotify audio features and decision trees in <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a>. Basically: <br><br>rap: speechy üó£Ô∏è<br>rock: can‚Äôt dance to it ü§ü<br>EDM: high tempo ‚è©<br>R&amp;B: long songs ‚è±Ô∏è<br>latin: very danceable üíÉ<br>pop: everything else.<a href="https://t.co/q57ZDdROf7">https://t.co/q57ZDdROf7</a> <a href="https://t.co/sfxRPKvpp2">pic.twitter.com/sfxRPKvpp2</a></p>&mdash; Kaylin Pavlik (@kaylinquest) <a href="https://twitter.com/kaylinquest/status/1213138536570015745?ref_src=twsrc%5Etfw">January 3, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 

The code includes a minimal machine learning style analysis with the following
steps:

* Import the songs data
* Split the songs data into training and test sets
* Build a tree model to classify songs into genres based on the song characteristics
* Assess accuracy of the model on the training and test sets
* Compute the accuracy of a model based on random guessing

## Getting started

The analysis purposefully contains various issues that make it difficult to
reproduce. Open the file `spotify.Rmd` by clicking on it in the RStudio Files
pane.

Next click the Knit button to re-build the document. The code fails
due to an error. Click on "Output" to see the lines in the Rmd
file where the error occurred.

Working together in small groups, reproduce the analysis by fixing the errors.
Below are hints on how to fix each error you will find.

## File paths

The first error you will encounter is below:

```
Quitting from lines 18-23 (spotify.Rmd) 
Error in file(file, "rt") : cannot open the connection
Calls: <Anonymous> ... withVisible -> eval -> eval -> read.csv -> read.table -> file
Execution halted
```

The function `read.csv()` is unable to open the data file. What's wrong with the
path to the file? Apply what you know about absolute and relative paths to
update the path and re-run the analysis.

## Missing package

The next error you encounter is:

```
Quitting from lines 37-40 (spotify.Rmd) 
Error in rpart(genre ~ ., data = spotifyTraining) : 
  could not find function "rpart"
Calls: <Anonymous> ... handle -> withCallingHandlers -> withVisible -> eval -> eval
Execution halted
```

The function `rpart()` can't be found. This can occur when you load a package
in the current R session, but forget to put the call to `library()` in the
script.

Based on the text above the code chunk, can you figure out which package needs
to be loaded?

## Renamed variable

The next error you encounter is:

```
Quitting from lines 62-67 (spotify.Rmd) 
Error in mean(spotifyTesting[, 1] == predictGuess) : 
  object 'predict_random' not found
Calls: <Anonymous> ... withCallingHandlers -> withVisible -> eval -> eval -> mean

Execution halted
```

R can't find the variable named `predictGuess`. Look at the surrounding code:
what do you think the name of this variable should be?

Renaming variables during an analysis can lead to these subtle errors. Since
both the original and updated versions of the variable are defined in the current
R session, the code will continue to run. But when you or someone else tries to
run the code in a clean R session, the code will unexpectedly fail.

## Compare results

Success! The analysis now runs. Compare your prediction results to that of your
partners' and/or re-run the analysis again. Are the results always identical?
Why not? What could you do if you wanted to publish these results and allow
others to exactly reproduce your findings?
